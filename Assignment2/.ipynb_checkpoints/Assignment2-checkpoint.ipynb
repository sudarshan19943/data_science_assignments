{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - 2\n",
    "\n",
    "## Author: Sudarshan Suresh Srikant - B00808452 - sudarshan.suresh@dal.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sudarshansuresh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#function to filter for VB/ADJ bigrams\n",
    "def filterTypes(ngram):\n",
    "    acceptable_types = ('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS')\n",
    "    second_type = ( 'JJ', 'JJR', 'JJS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups()\n",
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "3387\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism','talk.religion.misc','comp.graphics','sci.space']\n",
    "newsgroups = fetch_20newsgroups(categories=categories, subset='all')\n",
    "print(list(newsgroups.target_names))\n",
    "print(len(newsgroups.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. a. Tokenizing and performing POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('From', 'IN'), ('healta', 'JJ'), ('saturn', 'NN'), ('wwc', 'NN'), ('edu', 'NN'), ('Tammy', 'NNP'), ('R', 'NNP'), ('Healy', 'NNP'), ('Subject', 'NNP'), ('Re', 'NNP'), ('who', 'WP'), ('are', 'VBP'), ('we', 'PRP'), ('to', 'TO'), ('judge', 'VB'), ('Bobby', 'NNP'), ('Lines', 'NNPS'), ('Organization', 'NNP'), ('Walla', 'NNP'), ('Walla', 'NNP'), ('College', 'NNP'), ('Lines', 'NNP'), ('In', 'IN'), ('article', 'NN'), ('Apr', 'NNP'), ('ultb', 'JJ'), ('isc', 'NN'), ('rit', 'NN'), ('edu', 'NN'), ('snm', 'NN'), ('ultb', 'JJ'), ('isc', 'NN'), ('rit', 'NN'), ('edu', 'NN'), ('S', 'NNP'), ('N', 'NNP'), ('Mozumder', 'NNP'), ('writes', 'VBZ'), ('From', 'IN'), ('snm', 'NN'), ('ultb', 'JJ'), ('isc', 'NN'), ('rit', 'NN'), ('edu', 'NN'), ('S', 'NNP'), ('N', 'NNP'), ('Mozumder', 'NNP'), ('Subject', 'NNP'), ('Re', 'NNP'), ('who', 'WP'), ('are', 'VBP'), ('we', 'PRP'), ('to', 'TO'), ('judge', 'VB'), ('Bobby', 'NNP'), ('Date', 'NNP'), ('Wed', 'NNP'), ('Apr', 'NNP'), ('GMT', 'NNP'), ('In', 'IN'), ('article', 'NN'), ('healta', 'NN'), ('saturn', 'NN'), ('wwc', 'NN'), ('edu', 'NN'), ('healta', 'NN'), ('saturn', 'VBP'), ('wwc', 'JJ'), ('edu', 'NN'), ('TAMMY', 'NNP'), ('R', 'NNP'), ('HEALY', 'NNP'), ('writes', 'VBZ'), ('Bobby', 'NNP'), ('I', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('take', 'VB'), ('the', 'DT'), ('liberty', 'NN'), ('to', 'TO'), ('quote', 'VB'), ('from', 'IN'), ('a', 'DT'), ('Christian', 'JJ'), ('writer', 'NN'), ('named', 'VBN'), ('Ellen', 'NNP'), ('G', 'NNP'), ('White', 'NNP'), ('I', 'PRP'), ('hope', 'VBP'), ('that', 'IN'), ('what', 'WP'), ('she', 'PRP'), ('said', 'VBD'), ('will', 'MD'), ('help', 'VB'), ('you', 'PRP'), ('to', 'TO'), ('edit', 'VB'), ('your', 'PRP$'), ('remarks', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('group', 'NN'), ('in', 'IN'), ('the', 'DT'), ('future', 'NN'), ('Do', 'NNP'), ('not', 'RB'), ('set', 'VB'), ('yourself', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('standard', 'NN'), ('Do', 'NNP'), ('not', 'RB'), ('make', 'VB'), ('your', 'PRP$'), ('opinions', 'NNS'), ('your', 'PRP$'), ('views', 'NNS'), ('of', 'IN'), ('duty', 'NN'), ('your', 'PRP$'), ('interpretations', 'NNS'), ('of', 'IN'), ('scripture', 'NN'), ('a', 'DT'), ('criterion', 'NN'), ('for', 'IN'), ('others', 'NNS'), ('and', 'CC'), ('in', 'IN'), ('your', 'PRP$'), ('heart', 'NN'), ('condemn', 'VB'), ('them', 'PRP'), ('if', 'IN'), ('they', 'PRP'), ('do', 'VBP'), ('not', 'RB'), ('come', 'VB'), ('up', 'RP'), ('to', 'TO'), ('your', 'PRP$'), ('ideal', 'JJ'), ('Thoughts', 'NNPS'), ('Fromthe', 'NNP'), ('Mount', 'NNP'), ('of', 'IN'), ('Blessing', 'NNP'), ('p', 'NN'), ('I', 'PRP'), ('hope', 'VBP'), ('quoting', 'VBG'), ('this', 'DT'), ('doesn', 'NN'), ('t', 'WDT'), ('make', 'VBP'), ('the', 'DT'), ('atheists', 'NNS'), ('gag', 'VBP'), ('but', 'CC'), ('I', 'PRP'), ('think', 'VBP'), ('Ellen', 'NNP'), ('White', 'NNP'), ('put', 'VBD'), ('it', 'PRP'), ('better', 'JJR'), ('than', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('Tammy', 'VB'), ('Point', 'NNP'), ('Peace', 'NNP'), ('Bobby', 'NNP'), ('Mozumder', 'NNP'), ('My', 'NNP'), ('point', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('you', 'PRP'), ('set', 'VBP'), ('up', 'RP'), ('your', 'PRP$'), ('views', 'NNS'), ('as', 'IN'), ('the', 'DT'), ('only', 'JJ'), ('way', 'NN'), ('to', 'TO'), ('believe', 'VB'), ('Saying', 'VBG'), ('that', 'IN'), ('all', 'DT'), ('eveil', 'NN'), ('in', 'IN'), ('this', 'DT'), ('world', 'NN'), ('is', 'VBZ'), ('caused', 'VBN'), ('by', 'IN'), ('atheism', 'NN'), ('is', 'VBZ'), ('ridiculous', 'JJ'), ('and', 'CC'), ('counterproductive', 'JJ'), ('to', 'TO'), ('dialogue', 'VB'), ('in', 'IN'), ('this', 'DT'), ('newsgroups', 'NN'), ('I', 'PRP'), ('see', 'VBP'), ('in', 'IN'), ('your', 'PRP$'), ('posts', 'NNS'), ('a', 'DT'), ('spirit', 'NN'), ('of', 'IN'), ('condemnation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('atheists', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('newsgroup', 'NN'), ('bacause', 'IN'), ('they', 'PRP'), ('don', 'VBP'), ('t', 'JJ'), ('believe', 'VBP'), ('exactly', 'RB'), ('as', 'IN'), ('you', 'PRP'), ('do', 'VBP'), ('If', 'IN'), ('you', 'PRP'), ('re', 'VBP'), ('here', 'RB'), ('to', 'TO'), ('try', 'VB'), ('to', 'TO'), ('convert', 'VB'), ('the', 'DT'), ('atheists', 'NNS'), ('here', 'RB'), ('you', 'PRP'), ('re', 'VBP'), ('failing', 'VBG'), ('miserably', 'RB'), ('Who', 'WP'), ('wants', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('in', 'IN'), ('position', 'NN'), ('of', 'IN'), ('constantly', 'RB'), ('defending', 'VBG'), ('themselves', 'PRP'), ('agaist', 'JJ'), ('insulting', 'VBG'), ('attacks', 'NNS'), ('like', 'IN'), ('you', 'PRP'), ('seem', 'VBP'), ('to', 'TO'), ('like', 'VB'), ('to', 'TO'), ('do', 'VB'), ('I', 'PRP'), ('m', 'VB'), ('sorry', 'NN'), ('you', 'PRP'), ('re', 'VBP'), ('so', 'RB'), ('blind', 'IN'), ('that', 'IN'), ('you', 'PRP'), ('didn', 'VBP'), ('t', 'JJ'), ('get', 'VB'), ('the', 'DT'), ('messgae', 'NN'), ('in', 'IN'), ('the', 'DT'), ('quote', 'NN'), ('everyone', 'NN'), ('else', 'RB'), ('has', 'VBZ'), ('seemed', 'VBN'), ('to', 'TO'), ('Tammy', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "tokenizer = RegexpTokenizer(r'[^_\\W0-9]+') # Extract only alphabetical characters\n",
    "tokens = [tokenizer.tokenize(token) for token in newsgroups.data]\n",
    "pos_tagged_words = [nltk.pos_tag(token) for token in tokens] # Perform POS tagging on the tokens\n",
    "print(pos_tagged_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. b. Extract bigram collocations and apply techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(AALines, RAPTURE)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(hideously, expensiveMary)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(hetverschuldigde, bedrag)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(hestalking, aboutCarl)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(hermanrsilbigerSubject, ANSIAIIM)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(herjohan, engevik)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(herethanksFrom, MORIARTYNDSUVMBITNETSubject)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(hereping, ICMP)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(herd, Thevictor)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(herbertRetro, AerospaceFrom)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(henrikssronruunl, DAGPROGRAMMA)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(hemulnadakthseHello, networldIm)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(helpsYehUSCFrom, rjungclacbnewsdcbattcom)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(helpsStuart, DenmanstusoftuwashingtoneduNntpP...</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(helpsMark, LarsenmarklhunanrastekcomThis)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(helpsJason, WeilerweilejrpieduBTW)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(helpsDavid, ShuteEMAIL)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(helpsDaemonFrom, byabchpcutexasedu)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(helpout, hereComparing)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(helpmartti, toivakkamtoivakkabofiFrom)</td>\n",
       "      <td>19.802676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bigram        pmi\n",
       "0                                  (AALines, RAPTURE)  19.802676\n",
       "1                          (hideously, expensiveMary)  19.802676\n",
       "2                          (hetverschuldigde, bedrag)  19.802676\n",
       "3                             (hestalking, aboutCarl)  19.802676\n",
       "4                  (hermanrsilbigerSubject, ANSIAIIM)  19.802676\n",
       "5                                 (herjohan, engevik)  19.802676\n",
       "6       (herethanksFrom, MORIARTYNDSUVMBITNETSubject)  19.802676\n",
       "7                                    (hereping, ICMP)  19.802676\n",
       "8                                   (herd, Thevictor)  19.802676\n",
       "9                       (herbertRetro, AerospaceFrom)  19.802676\n",
       "10                   (henrikssronruunl, DAGPROGRAMMA)  19.802676\n",
       "11                  (hemulnadakthseHello, networldIm)  19.802676\n",
       "12         (helpsYehUSCFrom, rjungclacbnewsdcbattcom)  19.802676\n",
       "13  (helpsStuart, DenmanstusoftuwashingtoneduNntpP...  19.802676\n",
       "14         (helpsMark, LarsenmarklhunanrastekcomThis)  19.802676\n",
       "15                (helpsJason, WeilerweilejrpieduBTW)  19.802676\n",
       "16                           (helpsDavid, ShuteEMAIL)  19.802676\n",
       "17               (helpsDaemonFrom, byabchpcutexasedu)  19.802676\n",
       "18                           (helpout, hereComparing)  19.802676\n",
       "19            (helpmartti, toivakkamtoivakkabofiFrom)  19.802676"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PMI measure to check the occurance of a bigram\n",
    "text = ''.join(newsgroups.data)\n",
    "text = re.sub('[^ A-Za-z]+', '', text)\n",
    "word_tokens = nltk.wordpunct_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(word_tokens)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "pmi_df = pd.DataFrame(list(finder.score_ngrams(bigram_measures.pmi)), columns=['bigram', 'pmi']).sort_values(by='pmi', ascending=False)\n",
    "pmi_df.head(20).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>chi-sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(AAAA, BBBB)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(havewith, weaponsFYI)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(helmet, cams)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(helixmedunceduReplyTo, cptullymedunceduOrgani...</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(hejust, rattles)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(heikomathfuberlindemaggiaethzch, pubinetray)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(hedendaagsetechnologie, stelt)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(heatis, reradiated)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(hearken, onlyof)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(heardthe, gasp)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(healthinsurancedzkrizocfberkeleyedu, rockerFrom)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(hawksdwwlcorningcomFrom, looperccocaltechedu)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(hawkccwfccutexasedu, writesGreetings)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(havethose, oilfields)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(helpMike, Mattonemikemikukyedu)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(havemuch, nonindoeuropean)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(haveconfused, THEIR)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(haveCraig, Kolbs)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(havardnedbtihnoPostingFrontEnd, Winix)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(hasbehaved, inconsistently)</td>\n",
       "      <td>914533.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bigram    chi-sq\n",
       "0                                        (AAAA, BBBB)  914533.0\n",
       "1                              (havewith, weaponsFYI)  914533.0\n",
       "2                                      (helmet, cams)  914533.0\n",
       "3   (helixmedunceduReplyTo, cptullymedunceduOrgani...  914533.0\n",
       "4                                   (hejust, rattles)  914533.0\n",
       "5       (heikomathfuberlindemaggiaethzch, pubinetray)  914533.0\n",
       "6                     (hedendaagsetechnologie, stelt)  914533.0\n",
       "7                                (heatis, reradiated)  914533.0\n",
       "8                                   (hearken, onlyof)  914533.0\n",
       "9                                    (heardthe, gasp)  914533.0\n",
       "10  (healthinsurancedzkrizocfberkeleyedu, rockerFrom)  914533.0\n",
       "11     (hawksdwwlcorningcomFrom, looperccocaltechedu)  914533.0\n",
       "12             (hawkccwfccutexasedu, writesGreetings)  914533.0\n",
       "13                             (havethose, oilfields)  914533.0\n",
       "14                   (helpMike, Mattonemikemikukyedu)  914533.0\n",
       "15                        (havemuch, nonindoeuropean)  914533.0\n",
       "16                              (haveconfused, THEIR)  914533.0\n",
       "17                                 (haveCraig, Kolbs)  914533.0\n",
       "18            (havardnedbtihnoPostingFrontEnd, Winix)  914533.0\n",
       "19                       (hasbehaved, inconsistently)  914533.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chi-square measure to check the occurance of a bigram\n",
    "chi_sq_df = pd.DataFrame(list(finder.score_ngrams(bigram_measures.chi_sq)), columns=['bigram', 'chi-sq']).sort_values(by='chi-sq',ascending=False)\n",
    "chi_sq_df.head(20).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>filtered-freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(be, able)</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(is, available)</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(is, true)</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(are, available)</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(is, wrong)</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(be, more)</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(are, many)</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(is, possible)</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(be, wrong)</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(is, good)</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(be, true)</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(be, nice)</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(be, better)</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(is, right)</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(is, important)</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(is, much)</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(be, sure)</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(be, available)</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(is, free)</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(many, other)</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bigram  filtered-freq\n",
       "0         (be, able)       0.000187\n",
       "1    (is, available)       0.000174\n",
       "2         (is, true)       0.000095\n",
       "3   (are, available)       0.000077\n",
       "4        (is, wrong)       0.000062\n",
       "5         (be, more)       0.000060\n",
       "6        (are, many)       0.000058\n",
       "7     (is, possible)       0.000056\n",
       "8        (be, wrong)       0.000052\n",
       "9         (is, good)       0.000050\n",
       "10        (be, true)       0.000046\n",
       "11        (be, nice)       0.000044\n",
       "12      (be, better)       0.000043\n",
       "13       (is, right)       0.000039\n",
       "14   (is, important)       0.000039\n",
       "15        (is, much)       0.000037\n",
       "16        (be, sure)       0.000034\n",
       "17   (be, available)       0.000033\n",
       "18        (is, free)       0.000031\n",
       "19     (many, other)       0.000030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency with filter\n",
    "\n",
    "raw_freq_with_filter = pd.DataFrame(list(finder.score_ngrams(bigram_measures.raw_freq)), columns=['bigram', 'filtered-freq']).sort_values(by='filtered-freq',ascending=False)\n",
    "filtered_raw_freq_bi = raw_freq_with_filter[raw_freq_with_filter.bigram.map(lambda x: filterTypes(x))]\n",
    "filtered_raw_freq_bi.head(20).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>filtered-t-test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(be, able)</td>\n",
       "      <td>12.934417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(is, available)</td>\n",
       "      <td>11.842059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(is, true)</td>\n",
       "      <td>8.516765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(are, available)</td>\n",
       "      <td>7.917714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(is, wrong)</td>\n",
       "      <td>6.783755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(be, wrong)</td>\n",
       "      <td>6.579869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(are, many)</td>\n",
       "      <td>6.519642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(is, possible)</td>\n",
       "      <td>6.347374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(be, nice)</td>\n",
       "      <td>6.180663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(be, true)</td>\n",
       "      <td>5.993936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(be, more)</td>\n",
       "      <td>5.828218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(be, better)</td>\n",
       "      <td>5.714989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(is, important)</td>\n",
       "      <td>5.523404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(be, sure)</td>\n",
       "      <td>5.134317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(be, happy)</td>\n",
       "      <td>5.088474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(been, able)</td>\n",
       "      <td>5.029890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(many, other)</td>\n",
       "      <td>4.900336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(are, willing)</td>\n",
       "      <td>4.860809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(be, available)</td>\n",
       "      <td>4.740001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(be, interested)</td>\n",
       "      <td>4.723613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bigram  filtered-t-test\n",
       "0         (be, able)        12.934417\n",
       "1    (is, available)        11.842059\n",
       "2         (is, true)         8.516765\n",
       "3   (are, available)         7.917714\n",
       "4        (is, wrong)         6.783755\n",
       "5        (be, wrong)         6.579869\n",
       "6        (are, many)         6.519642\n",
       "7     (is, possible)         6.347374\n",
       "8         (be, nice)         6.180663\n",
       "9         (be, true)         5.993936\n",
       "10        (be, more)         5.828218\n",
       "11      (be, better)         5.714989\n",
       "12   (is, important)         5.523404\n",
       "13        (be, sure)         5.134317\n",
       "14       (be, happy)         5.088474\n",
       "15      (been, able)         5.029890\n",
       "16     (many, other)         4.900336\n",
       "17    (are, willing)         4.860809\n",
       "18   (be, available)         4.740001\n",
       "19  (be, interested)         4.723613"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-test with filter\n",
    "\n",
    "ttest_with_filter = pd.DataFrame(list(finder.score_ngrams(bigram_measures.student_t)), columns=['bigram','filtered-t-test']).sort_values(by='filtered-t-test', ascending=False)\n",
    "\n",
    "filtered_ttest_bi = ttest_with_filter[ttest_with_filter.bigram.map(lambda x: filterTypes(x))]\n",
    "filtered_ttest_bi.head(20).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. c. Top 20 results from each bigram measures\n",
    "\n",
    "According to the results above, we see that there is a good amount of overlap among the measures that are filtered. It makes sense to take the union of the filtered measures because the filtering is done based on which word types form a bigram.\n",
    "\n",
    "I have selected a filter function called filterTypes(x) which filters the bigrams to contain pairs of adjectives and verbs only. This way the top 20 combinations of (verb, adjective) pairs can be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. a. Text cleaning\n",
    "\n",
    "The text cleaning process has been carried out by removing non-alphabetic characters, stop words and by stemming the final result. The removal of non-alphabetic characters has been done in Cell #5 by providing the regular expression to the RegExpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: ['from', 'healta', 'saturn', 'wwc', 'edu', 'tammi', 'R', 'heali', 'subject', 'Re', 'judg', 'bobbi', 'line', 'organ', 'walla', 'walla', 'colleg', 'line', 'In', 'articl', 'apr', 'ultb', 'isc', 'rit', 'edu', 'snm', 'ultb', 'isc', 'rit', 'edu', 'S', 'N', 'mozumd', 'write', 'from', 'snm', 'ultb', 'isc', 'rit', 'edu', 'S', 'N', 'mozumd', 'subject', 'Re', 'judg', 'bobbi', 'date', 'wed', 'apr', 'gmt', 'In', 'articl', 'healta', 'saturn', 'wwc', 'edu', 'healta', 'saturn', 'wwc', 'edu', 'tammi', 'R', 'heali', 'write', 'bobbi', 'I', 'would', 'like', 'take', 'liberti', 'quot', 'christian', 'writer', 'name', 'ellen', 'G', 'white', 'I', 'hope', 'said', 'help', 'edit', 'remark', 'group', 'futur', 'Do', 'set', 'standard', 'Do', 'make', 'opinion', 'view', 'duti', 'interpret', 'scriptur', 'criterion', 'other', 'heart', 'condemn', 'come', 'ideal', 'thought', 'fromth', 'mount', 'bless', 'p', 'I', 'hope', 'quot', 'make', 'atheist', 'gag', 'I', 'think', 'ellen', 'white', 'put', 'better', 'I', 'could', 'tammi', 'point', 'peac', 'bobbi', 'mozumd', 'My', 'point', 'set', 'view', 'way', 'believ', 'say', 'eveil', 'world', 'caus', 'atheism', 'ridicul', 'counterproduct', 'dialogu', 'newsgroup', 'I', 'see', 'post', 'spirit', 'condemn', 'atheist', 'newsgroup', 'bacaus', 'believ', 'exactli', 'If', 'tri', 'convert', 'atheist', 'fail', 'miser', 'who', 'want', 'posit', 'constantli', 'defend', 'agaist', 'insult', 'attack', 'like', 'seem', 'like', 'I', 'sorri', 'blind', 'get', 'messga', 'quot', 'everyon', 'els', 'seem', 'tammi', 'from', 'jk', 'lehtori', 'cc', 'tut', 'fi', 'kouhia', 'juhana', 'subject', 'Re', 'more', 'gray', 'level', 'screen', 'organ', 'tamper', 'univers', 'technolog', 'line', 'distribut', 'inet', 'nntp']\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "filtered_tokens=[]\n",
    "for token in tokens:\n",
    "    for w in token:\n",
    "        if w not in stop_words:\n",
    "            filtered_tokens.append(w)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_tokens:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    \n",
    "print(f\"Stemmed tokens: {stemmed_words[0:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. b. and 2. c. SVM and Multinomial Naive Bayes\n",
    "\n",
    "The corpus has been converted into a bag-of-words tf-idf weighted vector representation using the TfidfVectorizer(). In order to remove the stop words and non-alhpabetic characters from the data, TfidfVectorizer takes stop_words and token_pattern as parameters to simplify the cleaning process. After the vectorization process has been complete, the vectors are trained onto two ML models as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC results with rbf kernel:\n",
      " [[  0   0 224   0]\n",
      " [  0   0 297   0]\n",
      " [  0   0 307   0]\n",
      " [  0   0 189   0]]\n",
      "Multinomial Naive Bayes Classifier results:\n",
      " [[221   0   2   1]\n",
      " [  1 293   3   0]\n",
      " [  0   7 300   0]\n",
      " [ 47   7  11 124]]\n",
      "\n",
      "Total Vocabulary length: 31500\n"
     ]
    }
   ],
   "source": [
    "data = np.array(newsgroups.data)\n",
    "labels = np.array(newsgroups.target)\n",
    "\n",
    "data = np.reshape(data, (data.shape[0],1))\n",
    "labels = np.reshape(labels, (labels.shape[0],1))\n",
    "\n",
    "newsgroups_train, newsgroups_test, newsgroups_target_train, newsgroups_target_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', token_pattern='[a-zA-Z]+')\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.ravel())\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.ravel())\n",
    "\n",
    "svm_clf = SVC(gamma='auto', kernel='rbf')\n",
    "svm_clf.fit(vectors, newsgroups_target_train.ravel())\n",
    "pred = svm_clf.predict(vectors_test)\n",
    "print(f\"SVC results with rbf kernel:\\n {metrics.confusion_matrix(newsgroups_target_test.ravel(), pred)}\")\n",
    "\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(vectors, newsgroups_target_train.ravel())\n",
    "pred = nb_clf.predict(vectors_test)\n",
    "print(f\"Multinomial Naive Bayes Classifier results:\\n {metrics.confusion_matrix(newsgroups_target_test.ravel(), pred)}\")\n",
    "\n",
    "print(f\"\\nTotal Vocabulary length: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC results with linear kernel:\n",
      " [[213   0   2   9]\n",
      " [  0 297   0   0]\n",
      " [  2   9 295   1]\n",
      " [ 11   4   3 171]]\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(gamma='auto', kernel='linear')\n",
    "svm_clf.fit(vectors, newsgroups_target_train.ravel())\n",
    "pred = svm_clf.predict(vectors_test)\n",
    "print(f\"SVC results with linear kernel:\\n {metrics.confusion_matrix(newsgroups_target_test.ravel(), pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Results:\n",
    "\n",
    "From the above cells, we can see that the accuracy of Multinomial Naive Bayes is much higher than that of SVM. The reason for this being that Naive Bayes is based on a probablistic model and works well when combined with TfidfVectorizer in calculating the probablities of occurance of each word. When vectorizied with TfidfVectorizer, the importance of every word is obtained along with the frequency of occurance. Naive Bayes then calculates the probability of occurance of these words by treating every word in the sentence to be independent from one another. This way a high accuracy with Naive Bayes was obtained [1].\n",
    "\n",
    "Initially when the model was trained on SVM with an 'rbf' kernel, the accuracy of the model seemed to be very less. From the confusion matrix with SVM(kernel='rbf), the classifier was mistaken in prediciting the right word class for every word. However, when the same SVM model had linear kernel, the accuracy of the model was almost the same as that of the multinomial naive bayes model. The reason for this is because of the 'kernel trick' [2]. The RBF kernel divides the data using a non-linear hyperplane in an infinite dimensional space, whereas a linear kernel will divide the data based on a linear hyperplane. Since the dataset is quite large, a linear kernel would be effective in classifiying the words, hence the reason for higher accuracy with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[^_\\W0-9]+')\n",
    "tokens_data = [tokenizer.tokenize(token) for token in newsgroups.data]\n",
    "pos_tagged_words_data = [nltk.pos_tag(token) for token in tokens_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_data = []\n",
    "nouns = ['NN','NNS','NNP', 'NNPS']\n",
    "\n",
    "noun_words = []\n",
    "\n",
    "for i in range(len(pos_tagged_words_data)):\n",
    "    noun_data = []\n",
    "    for j in range(len(pos_tagged_words_data[i])):\n",
    "        if (pos_tagged_words_data[i][j][1] in nouns):\n",
    "            noun_data.append(pos_tagged_words_data[i][j][0])\n",
    "    noun_words.append(noun_data)\n",
    "    \n",
    "noun_words = [\" \".join(record) for record in noun_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC results with rbf kernel:\n",
      " [[  0   0 224   0]\n",
      " [  0   0 297   0]\n",
      " [  0   0 307   0]\n",
      " [  0   0 189   0]]\n",
      "Multinomial Naive Bayes Classifier results:\n",
      " [[219   0   2   3]\n",
      " [  1 295   1   0]\n",
      " [  1   5 301   0]\n",
      " [ 42   5  14 128]]\n",
      "\n",
      "Vocabulary length of nouns: 23779\n"
     ]
    }
   ],
   "source": [
    "newsgroups_noun_train, newsgroups_noun_test, newsgroups_target_noun_train, newsgroups_target_noun_test = train_test_split(noun_words, newsgroups.target, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', token_pattern='[a-zA-Z]+')\n",
    "vectors = vectorizer.fit_transform(newsgroups_noun_train)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_noun_test)\n",
    "\n",
    "svm_clf = SVC(gamma='auto', kernel='rbf')\n",
    "svm_clf.fit(vectors, newsgroups_target_noun_train)\n",
    "pred = svm_clf.predict(vectors_test)\n",
    "print(f\"SVC results with rbf kernel:\\n {metrics.confusion_matrix(newsgroups_target_noun_test, pred)}\")\n",
    "\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(vectors, newsgroups_target_noun_train)\n",
    "pred = nb_clf.predict(vectors_test)\n",
    "print(f\"Multinomial Naive Bayes Classifier results:\\n {metrics.confusion_matrix(newsgroups_target_noun_test, pred)}\")\n",
    "\n",
    "print(f\"\\nVocabulary length of nouns: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC results with linear kernel:\n",
      " [[212   0   2  10]\n",
      " [  0 296   1   0]\n",
      " [  0   8 297   2]\n",
      " [ 12   2   2 173]]\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(gamma='auto', kernel='linear')\n",
    "svm_clf.fit(vectors, newsgroups_target_noun_train)\n",
    "pred = svm_clf.predict(vectors_test)\n",
    "print(f\"SVC results with linear kernel:\\n {metrics.confusion_matrix(newsgroups_target_noun_test, pred)}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. d. Accuracy Results for bag-of-words tf-idf weighted vector representation using only nouns\n",
    "\n",
    "As seen above the accuracies reported for tf-idf weighted noun vectors are almost the same as that of the entire corpus. However, the size of the vocabularies are different: Noun vocabulary size: 23779, Total vocabulary size: 31500. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1]\"Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation\", Medium, 2019. [Online]. Available: https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf. [Accessed: 16- Jul- 2019].\n",
    "\n",
    "[2]H. SVM, \"How to understand effect of RBF SVM\", Cross Validated, 2019. [Online]. Available: https://stats.stackexchange.com/questions/58585/how-to-understand-effect-of-rbf-svm. [Accessed: 16- Jul- 2019].\n",
    "\n",
    "[3]A. Syntax, L. Ryan, N. Kronenfeld and G. Maynard, \"Advanced Nested List Comprehension Syntax\", Stack Overflow, 2019. [Online]. Available: https://stackoverflow.com/questions/3766711/advanced-nested-list-comprehension-syntax. [Accessed: 16- Jul- 2019].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
